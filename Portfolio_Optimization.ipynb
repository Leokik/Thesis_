{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Leokik/Thesis_/blob/main/Portfolio_Optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from keras.callbacks import EarlyStopping\n",
        "import random"
      ],
      "metadata": {
        "id": "40cam4ODf5g-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tipo_di_ritorno = 1\n",
        "period_T = [30]\n",
        "period_P = [20]\n",
        "gamma_list = [0.1, 0.5, 1, 1.5, 2, 2.5, 3]"
      ],
      "metadata": {
        "id": "sEzZT2thf5d2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modello di ottimizzazione dei pesi, quindi sono pesi ottimi che ho considerando il primo periodo di Amp_Previsione\n",
        "def optimize_weights(n_titoli, gamma, meanLogRet, Sigma):  #qui prendo la media non una amtrice di rendimenti , ma un vettore di medie\n",
        "    def objective_function(w):\n",
        "        R = np.dot(meanLogRet,w)\n",
        "\n",
        "        V = np.sqrt(np.dot(w.T, np.dot(Sigma, w)))\n",
        "\n",
        "        return -1 * (R / V)\n",
        "\n",
        "    def check_sum_to_one(w):\n",
        "        return np.sum(w) - 1\n",
        "\n",
        "    constraints = ({'type': 'eq', 'fun': check_sum_to_one})\n",
        "    initial_weights = np.ones(n_titoli) / n_titoli\n",
        "    bounds = [(0, 1) for _ in range(n_titoli)]\n",
        "\n",
        "    optimal_weights = minimize(objective_function, initial_weights, method='SLSQP', bounds=bounds, constraints=constraints)\n",
        "    return optimal_weights.x\n"
      ],
      "metadata": {
        "id": "i6K6seoIf5WC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Queste mi servono per lo SR\n",
        "\n",
        "# Calcolo del rendimento atteso\n",
        "def calculate_expected_return(P, mu, x):\n",
        "    return 1 / P * (np.sum(np.dot(mu, x))) #mu Ã¨ una matrice di rendimaneti quindi la media la fa direttamente qui dnetro altrimenti mu sarebbe stato un vettore, non avrei messo /P\n",
        "\n",
        "# Calcolo della deviazione standard\n",
        "def calculate_standard_deviation(P, mu, x, Ex_ret):\n",
        "    return np.sqrt((1 / (P - 1)) * np.sum((np.dot(mu, x) - Ex_ret) ** 2))"
      ],
      "metadata": {
        "id": "E4shLkQof5TK"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creo un modello di ottimizzazione che ha come f.o. la nn\n",
        "# Modello di ottimizzazione dei pesi, con NN\n",
        "def opt_weights_nn(n_titoli,X):\n",
        "\n",
        "    def neural_network(x):\n",
        "\n",
        "        mean_NN = X.mean()\n",
        "        mean_NN = np.array(mean_NN)\n",
        "        conc_NN = np.concatenate([mean_NN, x])\n",
        "        #conc_NN = x #se uso solo x\n",
        "\n",
        "        input = conc_NN\n",
        "        input_norm = scaler.transform(input.reshape(1, -1))\n",
        "\n",
        "        # Pesi e bias del primo livello\n",
        "        W1, b1 = model.layers[1].get_weights()\n",
        "        # Pesi e bias del secondo livello\n",
        "        W2, b2 = model.layers[2].get_weights()\n",
        "\n",
        "        # Calcola l'output del primo livello senza applicare la funzione di attivazione\n",
        "        output_layer1 = tf.matmul(input_norm, W1) + b1\n",
        "        # Calcola l'output del secondo livello\n",
        "\n",
        "        # Calcola l'output del secondo livello\n",
        "        output_layer2 = tf.matmul(tf.tanh(output_layer1), W2) + b2\n",
        "\n",
        "        # Scala inversamente l'output\n",
        "        SR =  (scaler_SR.inverse_transform(output_layer2.numpy()))\n",
        "        return  SR * (-1)\n",
        "\n",
        "\n",
        "    def check_sum_to_one(x):\n",
        "        return np.sum(x) - 1\n",
        "\n",
        "    constraints = ({'type': 'eq', 'fun': check_sum_to_one})\n",
        "    initial_weights = np.ones(n_titoli) / n_titoli\n",
        "    bounds = [(0, 1) for _ in range(n_titoli)]\n",
        "\n",
        "    optimal_weights = minimize(neural_network, initial_weights, method='SLSQP', bounds=bounds, constraints=constraints)\n",
        "    return optimal_weights\n"
      ],
      "metadata": {
        "id": "jZNGGJGLf5QC"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_random_end_date():\n",
        "    start_date = datetime.datetime(2015, 1, 1)\n",
        "    end_date = datetime.datetime(2023, 1, 1)\n",
        "    random_days = random.randint(0, (end_date - start_date).days)\n",
        "    random_end_date = start_date + datetime.timedelta(days=random_days)\n",
        "    return random_end_date\n"
      ],
      "metadata": {
        "id": "FbwsA7qJf5M8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df = pd.DataFrame(columns=['T', 'P', 'gamma', 'end_date', 'Best_clo', 'Best_clo_val', 'best_eff', 'best_eff_val'])\n",
        "\n",
        "# Definizione della lista di simboli dei ticker\n",
        "tickers = ['BA', 'GOOGL', 'TSLA', 'PFE', 'AAPL', 'AMZN', 'MSFT', 'META', 'NVDA', 'V']\n",
        "\n",
        "# Creazione di un dizionario per memorizzare i dati scaricati\n",
        "stock_data = {}\n",
        "\n",
        "# Download dei dati per ciascun ticker e memorizzazione nel dizionario\n",
        "for ticker in tickers:\n",
        "    stock_data[ticker] = yf.download(ticker, end=datetime.datetime(2023, 11, 1))\n",
        "\n",
        "# Creazione di un DataFrame con i dati dei titoli\n",
        "stocks = pd.concat([stock_data[ticker]['Close'] for ticker in tickers], axis=1)\n",
        "\n",
        "# Assegnazione dei nomi alle colonne\n",
        "stocks.columns = tickers\n",
        "\n",
        "# Rimozione delle righe contenenti valori mancanti\n",
        "stocks_main = stocks.dropna()\n",
        "\n",
        "for T in period_T:\n",
        "    for P in period_P:\n",
        "        for gamma in gamma_list:\n",
        "            i = 0\n",
        "\n",
        "            while i < 15:\n",
        "                i = i + 1\n",
        "                print(i)\n",
        "\n",
        "                end_date = generate_random_end_date()\n",
        "                # Taglio del DataFrame per includere solo valori antecedenti a end_date\n",
        "                stocks = stocks_main.loc[stocks_main.index < end_date]\n",
        "\n",
        "                # Calcolo dei rendimenti e dei log-rendimenti\n",
        "                tipo_di_ritorno = 1  # Modifica a seconda del tuo requisito\n",
        "                returns = stocks / stocks.shift(tipo_di_ritorno)\n",
        "                logreturns = np.log(returns).dropna()\n",
        "\n",
        "                n_titoli = logreturns.shape[1]\n",
        "\n",
        "                X_test = logreturns.iloc[-(T + P):, :]\n",
        "                X_test_P = X_test.iloc[-P:, :]  # Per test\n",
        "\n",
        "                # inizializzo gli insiemi\n",
        "                X = []\n",
        "                SR = []\n",
        "\n",
        "                for t in range(len(logreturns) - (T + P)):\n",
        "\n",
        "                    logreturns_i = logreturns.iloc[t: T + P + t, :]  # creo finestre che si spostano di 1, ma che hanno la stessa ampiezza di T + P\n",
        "\n",
        "                    # costruisco X_\n",
        "                    rendimenti_T_per_X = logreturns_i.iloc[:T, :]\n",
        "\n",
        "                    rendimenti_P_per_x_otp = logreturns_i.iloc[-P:, :]\n",
        "\n",
        "                    # cose di P\n",
        "                    mean_i = rendimenti_P_per_x_otp.mean()\n",
        "                    Sigma_i = rendimenti_P_per_x_otp.cov()\n",
        "                    x_i_P = optimize_weights(n_titoli, gamma, mean_i, Sigma_i)\n",
        "\n",
        "                    # Cose di T\n",
        "                    mean_T_i = rendimenti_T_per_X.mean()\n",
        "                    Sigma_T_i = rendimenti_T_per_X.cov()\n",
        "                    x_i_T = optimize_weights(n_titoli, gamma, mean_i, Sigma_i)\n",
        "                    mean_T_i = np.array(mean_T_i)\n",
        "\n",
        "                    conc_i = np.concatenate([mean_T_i, x_i_P])  # alleno con media dei rendimenti su T e x_otp su P\n",
        "\n",
        "                    X.append(conc_i)\n",
        "\n",
        "                    # costruisco l'insieme target SR\n",
        "                    mu_i = rendimenti_P_per_x_otp\n",
        "                    Exr_i = calculate_expected_return(P, mu_i, x_i_P)\n",
        "                    Exd_i = calculate_standard_deviation(P, mu_i, x_i_P, Exr_i)\n",
        "\n",
        "                    SR_i = Exr_i / Exd_i\n",
        "                    SR.append(SR_i)\n",
        "\n",
        "                SR = np.array(SR)\n",
        "                X = np.array(X)\n",
        "\n",
        "                # Creare un array di indici\n",
        "                indici = np.arange(len(X))\n",
        "\n",
        "                # Mescolare gli indici\n",
        "                np.random.shuffle(indici)\n",
        "\n",
        "                # Applicare la permutazione agli array\n",
        "                X_mescolato = X[indici]\n",
        "                SR_mescolato = SR[indici]\n",
        "\n",
        "                # Normalizzo input ed output\n",
        "                scaler = StandardScaler()\n",
        "                X_normalized = scaler.fit_transform(X_mescolato)\n",
        "\n",
        "                scaler_SR = StandardScaler()\n",
        "                SR_normalized = scaler_SR.fit_transform(SR_mescolato.reshape(-1, 1)).flatten()\n",
        "\n",
        "                # Divisione del dataset\n",
        "                X_train, X_val, SR_train, SR_val = train_test_split(X_normalized, SR_normalized, test_size=0.2, random_state=42)\n",
        "\n",
        "                # Creazione del modello NN\n",
        "                model = tf.keras.Sequential([\n",
        "                    tf.keras.layers.Flatten(input_shape=(X_train.shape[1],)),\n",
        "                    tf.keras.layers.Dense(200, activation='tanh'),\n",
        "                    tf.keras.layers.Dense(1)\n",
        "                ])\n",
        "                # Imposta il callback EarlyStopping\n",
        "                early_stopping_callback = EarlyStopping(monitor='val_loss', patience=200, restore_best_weights=True)\n",
        "\n",
        "                # Compilazione e allenamento del modello con EarlyStopping\n",
        "                model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error'])\n",
        "                history = model.fit(X_train, SR_train, epochs=2000, batch_size=10, validation_data=(X_val, SR_val), callbacks=[early_stopping_callback], verbose=0)\n",
        "\n",
        "                # Valore si SR che vorrei in base al mio problema di ott [es. preferisco avere rendimenti piÃ¹ bassi, ma che siano meno variabili]\n",
        "                mean_true = X_test_P.mean()\n",
        "                Sigma_true = X_test_P.cov()\n",
        "\n",
        "                x_opt_test = optimize_weights(n_titoli, gamma, mean_true, Sigma_true)\n",
        "\n",
        "                Exr_test = calculate_expected_return(P, X_test_P, x_opt_test)\n",
        "                Exd_test = calculate_standard_deviation(P, X_test_P, x_opt_test, Exr_test)\n",
        "                SR_true_target = Exr_test / Exd_test\n",
        "\n",
        "                # Se avessi usato il metodo classico che SR avrei avuto?\n",
        "                X = X_test.iloc[:T, :]\n",
        "                mean = X.mean()\n",
        "                Sigma = X.cov()\n",
        "                x = optimize_weights(n_titoli, gamma, mean, Sigma)  # calcolo le x sulla base dello storico che ho\n",
        "\n",
        "                Exr_clas = calculate_expected_return(P, X_test_P, x)\n",
        "                Exd_clas = calculate_standard_deviation(P, X_test_P, x, Exr_test)\n",
        "                SR_classic = Exr_clas / Exd_clas\n",
        "\n",
        "                # Approccio Neural\n",
        "                x_NN = opt_weights_nn(n_titoli, X).x\n",
        "\n",
        "                Exr_NN = calculate_expected_return(P, X_test_P, x_NN)\n",
        "                Exd_NN = calculate_standard_deviation(P, X_test_P, x_NN, Exr_NN)\n",
        "                SR_NN = Exr_NN / Exd_NN\n",
        "\n",
        "                # Graficare la frontiera\n",
        "                mean_true = X_test_P.mean()\n",
        "                Sigma_true = X_test_P.cov()\n",
        "\n",
        "                def minimizeMyVolatility(w):\n",
        "                    w = np.array(w)\n",
        "                    V = np.sqrt(np.dot(w.T, np.dot(Sigma_true, w)))\n",
        "                    return -1 * (Exr_NN / V)\n",
        "\n",
        "                def checkSumToOne(w):\n",
        "                    return np.sum(w) - 1\n",
        "\n",
        "                def iniz_w0(n_titoli):\n",
        "                    w0 = []\n",
        "                    for i in range(n_titoli):\n",
        "                        w0.append(1 / n_titoli)\n",
        "                    return w0\n",
        "\n",
        "                def bounds(n_titoli):\n",
        "                    bounds = []\n",
        "                    for i in range(n_titoli):\n",
        "                        bounds.append((0, 1))\n",
        "                    return bounds\n",
        "\n",
        "                constraints = {'type': 'eq', 'fun': checkSumToOne}\n",
        "\n",
        "                opt = minimize(minimizeMyVolatility, iniz_w0(n_titoli), method='SLSQP', bounds=bounds(n_titoli),\n",
        "                               constraints=constraints)\n",
        "                x_otp_NN = opt.x\n",
        "                V_otp_NN = np.sqrt(np.dot(x_otp_NN.T, np.dot(Sigma_true, x_otp_NN)))\n",
        "\n",
        "                # Graficare la frontiera\n",
        "                mean_true = X_test_P.mean()\n",
        "                Sigma_true = X_test_P.cov()\n",
        "\n",
        "                def minimizeMyVolatility(w):\n",
        "                    w = np.array(w)\n",
        "                    V = np.sqrt(np.dot(w.T, np.dot(Sigma_true, w)))\n",
        "                    return -1 * (Exr_clas / V)\n",
        "\n",
        "                def checkSumToOne(w):\n",
        "                    return np.sum(w) - 1\n",
        "\n",
        "                def iniz_w0(n_titoli):\n",
        "                    w0 = []\n",
        "                    for i in range(n_titoli):\n",
        "                        w0.append(1 / n_titoli)\n",
        "                    return w0\n",
        "\n",
        "                def bounds(n_titoli):\n",
        "                    bounds = []\n",
        "                    for i in range(n_titoli):\n",
        "                        bounds.append((0, 1))\n",
        "                    return bounds\n",
        "\n",
        "                constraints = {'type': 'eq', 'fun': checkSumToOne}\n",
        "\n",
        "                opt = minimize(minimizeMyVolatility, iniz_w0(n_titoli), method='SLSQP', bounds=bounds(n_titoli),\n",
        "                               constraints=constraints)\n",
        "                x_otp_classic = opt.x\n",
        "                Vol_otp_classic = np.sqrt(np.dot(x_otp_classic.T, np.dot(Sigma_true, x_otp_classic)))\n",
        "\n",
        "                # Coordinate dei punti\n",
        "                red_coords = np.array([Exd_test, Exr_test])\n",
        "                violet_coords = np.array([Exd_clas, Exr_clas])\n",
        "                black_coords = np.array([Exd_NN, Exr_NN])\n",
        "                green_coords = np.array([V_otp_NN, Exr_NN])\n",
        "                pink_coords = np.array([Vol_otp_classic, Exr_clas])\n",
        "\n",
        "                # Calcola le distanze\n",
        "                distanza_red_black = np.linalg.norm(red_coords - black_coords)\n",
        "                distanza_red_violet = np.linalg.norm(red_coords - violet_coords)\n",
        "                distanza_black_green = np.linalg.norm(black_coords - green_coords)\n",
        "                distanza_pink_violet = np.linalg.norm(pink_coords - violet_coords)\n",
        "\n",
        "                # PiÃ¹ vicino al target\n",
        "                if distanza_red_black <= distanza_red_violet:\n",
        "                    Best_clo = 'Neural'\n",
        "                    Best_clo_vall = distanza_red_black\n",
        "\n",
        "                else:\n",
        "                    Best_clo = 'Classic'\n",
        "                    Best_clo_vall = distanza_red_violet\n",
        "\n",
        "                if distanza_black_green <= distanza_pink_violet:\n",
        "                    best_eff = 'Neural'\n",
        "                    best_eff_vall = distanza_black_green\n",
        "\n",
        "                else:\n",
        "                    best_eff = 'Classic'\n",
        "                    best_eff_vall = distanza_pink_violet\n",
        "\n",
        "                result_row = pd.DataFrame({'T': [T], 'P': [P], 'gamma': [gamma], 'end_date': [end_date],\n",
        "                                           'Best_clo': [Best_clo], 'Best_clo_val': [Best_clo_vall],\n",
        "                                           'best_eff': [best_eff], 'best_eff_val': [best_eff_vall]})\n",
        "                results_df = pd.concat([results_df, result_row], ignore_index=True)\n",
        "                print(results_df)\n"
      ],
      "metadata": {
        "id": "td33C-UXf5I-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df"
      ],
      "metadata": {
        "id": "6RkyMm31f441"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ia30F9uIkZlb"
      },
      "execution_count": 5,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}